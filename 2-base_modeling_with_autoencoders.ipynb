{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971a139b-8ce8-4d1f-b541-72e24416a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adaaa23-5bd7-423c-a9ee-68f5b6783118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from bokeh.models import ColumnDataSource, Band, HoverTool\n",
    "from bokeh.plotting import figure, show\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.measuring_performance import (\n",
    "    get_prediction, plot_histogram_by_class, plot_loss_per_epoch, plot_pr_curve, plot_roc_curve)\n",
    "from utils.misc import build_files_list, dump_pickle, load_pickle\n",
    "from utils.sound_utils import extract_signal_features, generate_dataset, load_sound_file\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "bokeh.io.output_notebook()\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c27992c-448d-4fcb-82df-005cb6b01da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78552a56-23e4-4295-a178-b625ac2c4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca7bdf-bbf5-48e8-8542-4bb028fd1094",
   "metadata": {},
   "source": [
    "The boolean variable `COMBINE_MACHINE_ID` allows you to decide whether to perform modeling separately for each machine ID, or combine all data from the same machine type to perform modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34adbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/mimii-anomaly-detection'\n",
    "IMAGE_PATH = './img'\n",
    "MODEL_PATH = './models'\n",
    "MERGE_MACHINE_ID = True\n",
    "\n",
    "os.makedirs(os.path.join(DATA_PATH, 'dataset'), exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "file_paths = sorted(glob.glob(DATA_PATH + '/*/*' if MERGE_MACHINE_ID else DATA_PATH + '/*/*/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870f533-d3a3-4517-a965-3af32fe38f6f",
   "metadata": {},
   "source": [
    "# Data Splitting and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404129ca-a017-40f8-85b8-9a9244d561cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db: 6dB, machine type: fan\n"
     ]
    }
   ],
   "source": [
    "file_path = file_paths[0] # the first machine ID or machine type selected \n",
    "file_path_split = file_path.split('/')\n",
    "suffix = '_'.join(['', file_path_split[-1], file_path_split[-2]])\n",
    "\n",
    "if MERGE_MACHINE_ID:\n",
    "    print('db: {}, machine type: {}'.format(file_path_split[-2], file_path_split[-1]))\n",
    "\n",
    "else:\n",
    "    print('db: {}, machine type: {}, machine id: {}'.format(file_path_split[-3], file_path_split[-2], file_path_split[-1]))\n",
    "    suffix = '_'.join([suffix, file_path_split[-3]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83bb51-93f6-4adf-a435-1916f9aa1c25",
   "metadata": {},
   "source": [
    "Because unsupervised learning is used, all anomalous labels are assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04fe076f-09a8-4e64-8716-e35fe88e66f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 3260 signals including abnormal 0 signals, but test set has 2290 signals including abnormal 1475 signals.\n"
     ]
    }
   ],
   "source": [
    "normal_files, abnormal_files = build_files_list(root_dir=file_path)\n",
    "normal_labels = np.zeros(len(normal_files))\n",
    "abnormal_labels = np.ones(len(abnormal_files))\n",
    "\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(normal_files, normal_labels, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "test_files = np.concatenate((test_files, abnormal_files), axis=0)\n",
    "test_labels = np.concatenate((test_labels, abnormal_labels), axis=0)\n",
    "\n",
    "test_indices = np.arange(len(test_files))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "test_files = test_files[test_indices]\n",
    "test_labels = test_labels[test_indices]\n",
    "\n",
    "print('Train set has {} signals including abnormal {:.0f} signals, but test set has {} signals including abnormal {:.0f} signals.'.format(\n",
    "    train_labels.shape[0], train_labels.sum(), test_labels.shape[0], test_labels.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd514d6-75e7-4199-8d90-3611f8f79934",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train_files': train_files,\n",
    "    'test_files': test_files,\n",
    "    'train_labels': train_labels,\n",
    "    'test_labels': test_labels\n",
    "}\n",
    "\n",
    "for key, values in dataset.items():\n",
    "    file_name = os.path.join(DATA_PATH, 'dataset', key + suffix + '.txt')\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in values:\n",
    "            f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06251cae-d042-44cb-9590-eb71052a5a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data already exists, loading from file...\n",
      "Train data has a (1007340, 320) shape.\n"
     ]
    }
   ],
   "source": [
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "n_mels = 64\n",
    "frames = 5\n",
    "\n",
    "train_data_path = os.path.join(DATA_PATH, 'dataset', 'train_data' + suffix + '.pkl')\n",
    "\n",
    "if os.path.exists(train_data_path):\n",
    "    print('Train data already exists, loading from file...')\n",
    "    train_data = load_pickle(train_data_path)\n",
    "        \n",
    "else:\n",
    "    train_data = generate_dataset(train_files, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, frames=frames)\n",
    "    print('Saving train data to disk...')\n",
    "    dump_pickle(train_data_path, train_data)\n",
    "    print('Done.')\n",
    "\n",
    "print(f'Train data has a {train_data.shape} shape.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa3e40-6238-4a80-949a-d25f11c7c737",
   "metadata": {},
   "source": [
    "# Model Training and Prediction with AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c0bfa5-09cd-4a5c-930a-bfca91ed7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(input_dims, model_name=None):\n",
    "    input_layer = Input(shape=(input_dims,))\n",
    "    h = Dense(64, activation='relu')(input_layer)\n",
    "    h = Dense(64, activation='relu')(h)\n",
    "    h = Dense(8, activation='relu')(h)\n",
    "    h = Dense(64, activation='relu')(h)\n",
    "    h = Dense(64, activation='relu')(h)\n",
    "    h = Dense(input_dims, activation=None)(h)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=h, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65806b41-edab-496a-94b2-abcf5c9b20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aldente0630/anaconda3/envs/mimii-anomaly-detection/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"AutoEncoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 320)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                20544     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 320)               20800     \n",
      "=================================================================\n",
      "Total params: 50,760\n",
      "Trainable params: 50,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_name = 'AutoEncoder'\n",
    "model = autoencoder(n_mels * frames, model_name=model_name)\n",
    "print(model.summary())\n",
    "\n",
    "gpu_count = len(get_available_gpus())\n",
    "if gpu_count > 1:\n",
    "    model = multi_gpu_model(model, gpus=gpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1f5bb-828e-406f-97de-9a2933417d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-03),\n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=False,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)],\n",
    "    validation_split=0.1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model.save(os.path.join(MODEL_PATH, model_name +  suffix + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec0991-d2b9-4369-8e2d-1bd2ecd3bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_per_epoch(\n",
    "    history, model_name=model_name, file_name=os.path.join(IMAGE_PATH, 'ae_model_loss.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c503335-ed82-468a-89d7-1ca6cc0abe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_errors = []\n",
    "\n",
    "for index in tqdm(range(len(test_files))):\n",
    "    signal, sr = load_sound_file(test_files[index])\n",
    "    \n",
    "    features = extract_signal_features(\n",
    "        signal, \n",
    "        sr, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels, \n",
    "        frames=frames\n",
    "    )\n",
    "    \n",
    "    predictions = model.predict(features)\n",
    "    mse = np.mean(np.mean(np.square(features - predictions), axis=1))\n",
    "    recon_errors.append(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c6e87-716e-4543-9c08-b93338ac35c7",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923d9e7-6b08-4719-b6c2-3419ee0e7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = np.column_stack((range(len(recon_errors)), recon_errors))\n",
    "score_false = stack[test_labels == 0][:, 1]\n",
    "score_true = stack[test_labels == 1][:, 1]\n",
    "\n",
    "plot_histogram_by_class(\n",
    "    score_false, score_true, bins=[20, 30], model_name=model_name, file_name=os.path.join(IMAGE_PATH, 'ae_recon_error_dist.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cab09b-1956-45a2-8340-c7e6da5ca53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_min = 3.0\n",
    "thr_max = 8.0\n",
    "\n",
    "p = figure(plot_width=600, plot_height=400, title='{} - Threshold Range Exploration'.format(model_name), x_axis_label='Samples', y_axis_label='Reconstruction Error')\n",
    "\n",
    "source = ColumnDataSource(dict(index=stack[test_labels == 0][:, 0], error=stack[test_labels == 0][:, 1]))\n",
    "p.scatter('index', 'error', fill_alpha=0.6, fill_color='crimson', line_color=None, legend_label='Normal Signals', source=source)\n",
    "\n",
    "source = ColumnDataSource(dict(index=stack[test_labels == 1][:, 0], error=stack[test_labels == 1][:, 1]))\n",
    "p.scatter('index', 'error', fill_alpha=0.6, fill_color='indigo', line_color=None, legend_label='Abnormal Signals', source=source)\n",
    "\n",
    "source = ColumnDataSource(data=dict(index=stack[:, 0], thr_min=np.repeat(thr_min, stack.shape[0]), thr_max=np.repeat(thr_max, stack.shape[0])))\n",
    "\n",
    "band = Band(base='index', lower='thr_min', upper='thr_max', level='underlay', \n",
    "            fill_alpha=0.1, fill_color='magenta', line_color='darkmagenta', line_width=1.0, source=source)\n",
    "p.add_layout(band)\n",
    "\n",
    "p.legend.label_text_font_size = '8pt'\n",
    "p.title.align = 'center'\n",
    "p.title.text_font_size = '12pt'\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('index', '@index'), ('error', '@error')]))\n",
    "show(p)\n",
    "\n",
    "p.output_backend = 'svg'\n",
    "_ = export_svgs(p, filename=os.path.join(IMAGE_PATH, 'ae_thr_range_expl.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21635c82-cd0a-4bac-b205-033d46ae8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3.0\n",
    "preds = get_prediction(stack[:, 1], threshold=threshold)\n",
    "\n",
    "print('<{0}> Accuracy: {1:.2%}, Precision: {2:.2%}, Recall: {3:.2%}, F1: {4:.2%}'.format(\n",
    "    model_name, accuracy_score(test_labels, preds), precision_score(test_labels, preds), \n",
    "    recall_score(test_labels, preds), f1_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c6acd-f64d-43ef-94ac-406e9d38ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(\n",
    "    roc_curve(test_labels, recon_errors), roc_auc_score(test_labels, recon_errors), \n",
    "    model_name=model_name, file_name=os.path.join(IMAGE_PATH, 'ae_roc_curve.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfe6d4-35cc-473a-968a-71dbe38551f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(\n",
    "    precision_recall_curve(test_labels, recon_errors), average_precision_score(test_labels, recon_errors), \n",
    "    model_name=model_name, file_name=os.path.join(IMAGE_PATH, 'ae_pr_curve.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c9c98-432e-4158-89df-825cf74f2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_types(df, ground_truth_col='Ground Truth', prediction_col='Prediction', normal_label=0.0, anomaly_label=1.0):\n",
    "    \"\"\"\n",
    "    Compute false positive and false negatives columns based on the prediction\n",
    "    and ground truth columns from a dataframe.\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        df (dataframe)\n",
    "            Dataframe where the ground truth and prediction columns are available\n",
    "        ground_truth_col (string)\n",
    "            Column name for the ground truth values. Defaults to \"Ground Truth\"\n",
    "        prediction_col (string)\n",
    "            Column name for the predictied values. Defaults to \"Prediction\"\n",
    "        normal_label (object)\n",
    "            Value taken by a normal value. Defaults to 0.0\n",
    "        anomaly_label (object)\n",
    "            Value taken by an abnormal value. Defaults to 1.0\n",
    "            \n",
    "    RETURNS\n",
    "    =======\n",
    "        df (dataframe)\n",
    "            An updated dataframe with 4 new binary columns for TP, TN, FP and FN.\n",
    "    \"\"\"\n",
    "    df['TP'] = 0\n",
    "    df['TN'] = 0\n",
    "    df['FP'] = 0\n",
    "    df['FN'] = 0\n",
    "    df.loc[(df[ground_truth_col] == df[prediction_col]) & (df[ground_truth_col] == normal_label), 'TP'] = 1\n",
    "    df.loc[(df[ground_truth_col] == df[prediction_col]) & (df[ground_truth_col] == anomaly_label), 'TN'] = 1\n",
    "    df.loc[(df[ground_truth_col] != df[prediction_col]) & (df[ground_truth_col] == normal_label), 'FP'] = 1\n",
    "    df.loc[(df[ground_truth_col] != df[prediction_col]) & (df[ground_truth_col] == anomaly_label), 'FN'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_curves(FP, FN, nb_samples, threshold_min, threshold_max, threshold_step):\n",
    "    \"\"\"\n",
    "    Plot the false positives and false negative samples number depending on a given threshold.\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        FP (dataframe)\n",
    "            Number of false positives depending on the threshold\n",
    "        FN (dataframe)\n",
    "            Number of false negatives depending on the threshold\n",
    "        threshold_min (float)\n",
    "            Minimum threshold to plot for\n",
    "        threshold_max (float)\n",
    "            Maximum threshold to plot for\n",
    "        threshold_step (float)\n",
    "            Threshold step to plot these curves\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    min_FN = np.argmin(FN)\n",
    "    min_FP = np.where(FP == np.min(FP))[0][-1]\n",
    "    plot_top = max(FP + FN) + 1\n",
    "\n",
    "    # Grid customization:\n",
    "    major_ticks = np.arange(threshold_min, threshold_max, 1.0 * threshold_step)\n",
    "    minor_ticks = np.arange(threshold_min, threshold_max, 0.2 * threshold_step)\n",
    "    ax.set_xticks(major_ticks);\n",
    "    ax.set_xticks(minor_ticks, minor=True);\n",
    "    ax.grid(which='minor', alpha=0.5)\n",
    "    ax.grid(which='major', alpha=1.0, linewidth=1.0)\n",
    "    \n",
    "    # Plot false positives and false negatives curves\n",
    "    plt.plot(np.arange(threshold_min, threshold_max + threshold_step, threshold_step), FP, label='False positive', color='tab:red')\n",
    "    plt.plot(np.arange(threshold_min, threshold_max + threshold_step, threshold_step), FN, label='False negative', color='tab:green')\n",
    "\n",
    "    # Finalize the plot with labels and legend:\n",
    "    plt.xlabel('Reconstruction error threshold (%)', fontsize=16)\n",
    "    plt.ylabel('# Samples', fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (4,3), fontsize=14):\n",
    "    \"\"\"\n",
    "    Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix,\n",
    "    as a heatmap.\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        confusion_matrix (numpy.ndarray)\n",
    "            The numpy.ndarray object returned from a call to \n",
    "            sklearn.metrics.confusion_matrix. Similarly constructed \n",
    "            ndarrays can also be used.\n",
    "        class_names (list)\n",
    "            An ordered list of class names, in the order they index the given\n",
    "            confusion matrix.\n",
    "        figsize (tuple)\n",
    "            A 2-long tuple, the first value determining the horizontal size of\n",
    "            the ouputted figure, the second determining the vertical size.\n",
    "            Defaults to (10,7).\n",
    "        fontsize: (int)\n",
    "            Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    RETURNS\n",
    "    =======\n",
    "        matplotlib.figure.Figure: The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    # Build a dataframe from the confusion matrix passed as argument:\n",
    "    df_cm = pd.DataFrame(confusion_matrix, \n",
    "                         index=class_names, \n",
    "                         columns=class_names)\n",
    "    \n",
    "    # Plot the confusion matrix:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", annot_kws={\"size\": 16}, cmap='viridis')\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "        \n",
    "    # Figure customization:\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label', fontsize=16)\n",
    "    plt.xlabel('Predicted label', fontsize=16)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a2e43-83ad-4a45-9465-d585a0152a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "thresholds = np.arange(thr_min, thr_max + threshold_step, threshold_step)\n",
    "\n",
    "df = pd.DataFrame(columns=['Signal', 'Ground Truth', 'Prediction', 'Reconstruction Error'])\n",
    "df['Signal'] = test_files\n",
    "df['Ground Truth'] = test_labels\n",
    "df['Reconstruction Error'] = recon_errors\n",
    "\n",
    "FN = []\n",
    "FP = []\n",
    "for th in thresholds:\n",
    "    df.loc[df['Reconstruction Error'] <= th, 'Prediction'] = 0.0\n",
    "    df.loc[df['Reconstruction Error'] > th, 'Prediction'] = 1.0\n",
    "    df = generate_error_types(df)\n",
    "    FN.append(df['FN'].sum())\n",
    "    FP.append(df['FP'].sum())\n",
    "        \n",
    "plot_curves(FP, FN, nb_samples=df.shape[0], threshold_min=thr_min, threshold_max=thr_max, threshold_step=threshold_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cb742-34f9-47ca-81b4-5610ecef8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "th = 6.275\n",
    "df.loc[df['Reconstruction Error'] <= th, 'Prediction'] = 0.0\n",
    "df.loc[df['Reconstruction Error'] > th, 'Prediction'] = 1.0\n",
    "df['Prediction'] = df['Prediction'].astype(np.float32)\n",
    "df = generate_error_types(df)\n",
    "tp = df['TP'].sum()\n",
    "tn = df['TN'].sum()\n",
    "fn = df['FN'].sum()\n",
    "fp = df['FP'].sum()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "df['Ground Truth'] = 1 - df['Ground Truth']\n",
    "df['Prediction'] = 1 - df['Prediction']\n",
    "print_confusion_matrix(confusion_matrix(df['Ground Truth'], df['Prediction']), class_names=['abnormal', 'normal']);\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
